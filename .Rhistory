#verbose = 2,
nthreads = 4
)
plot(m_xgb1)
plot(varImp(m_xgb1))
# Boosted regression tree
dt_xgb2 <- dt
mortgage_yn <- dt_xgb2$mortgage_yn
target <- ifelse(dt_xgb2$mortgage_yn == 'Y',1,0)
dt_xgb2$mortgage_yn <- NULL
dt_xgb2$current_balance_eur <- NULL
dt_xgb2 <- as.matrix(dt_xgb2)
m_xgb2<- train(x = dt_xgb2,
y = mortgage_yn,
trControl = fitControl1,
tuneGrid = g_xgb,
method = 'xgbTree',
preProcess = c('center','scale'),
#verbose = 2,
nthreads = 4
)
plot(m_xgb2)
plot(varImp(m_xgb2))
str(dt)
knitr::opts_chunk$set(echo = TRUE)
library(data.table)
library(lubridate)
library(dplyr)
library(skimr)
library(magrittr)
library(ggplot2)
#Load training data ----
train <- fread('../data/retail_data.csv', sep=';', dec=',')
names(train) <- tolower(names(train))
train[, ':=' (current_address_date = as.Date(current_address_date)
, current_job_date = as.Date(current_job_date)
, current_with_bank_date = as.Date(current_with_bank_date)
)]
meta <- data.frame(skim(train))
print(skim(train))
#__ Categorical variables ----
cols <- unique(meta[meta$type == 'character',]$variable)
gd <- melt(train[, c('cocunut', cols), with=F], id.vars = 'cocunut')
ggplot(data=gd, aes(x=value)) +
geom_bar() +
facet_wrap(. ~ variable, scales = 'free', ncol = 2)
#__ Numeric variables ----
cols <- unique(meta[meta$type == 'integer',]$variable)
gd <- melt(train[, cols, with=F], id.vars = 'cocunut')
ggplot(data=gd[!is.na(value),], aes(x=value)) +
geom_histogram(binwidth = 2) +
facet_wrap(. ~ variable, scales = 'free', ncol = 2)
cols <- unique(meta[meta$type == 'numeric',]$variable)
gd <- melt(train[, c('cocunut','education',cols), with=F]
, id.vars = c('cocunut','education'))
gd[, q75 := quantile(value, .75), by = .(variable, education)]
gd[, q := ifelse(value>q75, 'Top', 'Bottom')]
ggplot(data=gd[q=='Bottom',], aes(x=education, y=value)) +
geom_boxplot() +
coord_flip() +
facet_grid(education ~ variable, scales = 'free') +
ggtitle('Excluded top quartile')
ggplot(data=gd[q=='Top',], aes(x=education, y=value)) +
geom_boxplot() +
coord_flip() +
facet_grid(education ~ variable, scales = 'free') +
ggtitle('Top quartile')
train[, .(education, cust_income, current_balance_eur)] %>%
dplyr::group_by(education) %>%
skim()
##__ Dates ----
cols <- unique(meta[meta$type == 'Date',]$variable)
gd <- melt(train[, c('cocunut', cols), with=F], id.vars = 'cocunut')
gd$year <- year(gd$value)
table(gd$variable, gd$year)
ggplot(data=gd[year<=2016,], aes(x=year)) +
geom_histogram(binwidth = 2) +
facet_wrap(. ~ variable, scales = 'free', ncol = 2)
##__ Interval scales ----
feature_intervals <- function(x) {
x[,
# Current date
current_date := current_with_bank_date %m+% years(years_with_bank)
][, ':='
# New time interval fatures
(birth_date = current_date - years(age)
, mths_in_job = interval(current_job_date, current_date) %/% months(1)
, mths_at_address = interval(current_address_date, current_date) %/% months(1)
, mths_with_bank = years_with_bank * 12
, age_adj = pmin(age_at_origination, age, na.rm = T))
][]
}
##__ Update to missing ----
feature_set_NA <- function(x){
x[, ':='
# Clean negative intervals and age < address date interval
(mths_at_address = ifelse(mths_at_address > age * 12
, age * 12,
ifelse(mths_at_address < 0, NA
, mths_at_address))
, mths_in_job = ifelse(mths_in_job < 0, NA
, mths_in_job)
, martial_status = ifelse(martial_status == '*noval*', 'noval'
, martial_status))][]
}
train <- train %>%
feature_intervals() %>%
feature_set_NA()
train <- train[, .(# Target variable
mortgage_yn
# Demographics
, martial_status, education, employment, gender
# Financial situation
, cust_income, current_balance_eur
# Lifecycle
, age_adj, mths_with_bank, mths_in_job, mths_at_address)]
print(skim(train))
cols <- c('martial_status', 'education', 'employment', 'gender')
cols <- merge(cols, cols)
cols <- cols[cols$x != cols$y,]
cramers_v <- function(x, y){
r <- sqrt(chisq.test(x, y, simulate.p.value = T)$statistic /
(length(x) * (min(length(unique(x)), length(unique(y))) - 1)))
return(as.numeric(r))
}
cols$cv <- apply(cols, 1
, function(x){cramers_v(train[[x[1]]], train[[x[2]]])})
cols
ggplot(data=train[martial_status != 'noval' & education != 'OTH'
& current_balance_eur * cust_income != 0,]) +
geom_point(aes(x=cust_income, y=current_balance_eur
, group=mortgage_yn, color=mortgage_yn)) +
scale_x_log10() +
facet_grid(education ~ martial_status, scales = 'free')
library(corrplot)
gd <- train[, .(cust_income, current_balance_eur, age_adj, mths_with_bank
, mths_in_job, mths_at_address)]
gd <- gd[!is.na(mths_at_address) & !is.na(mths_in_job),]
gd <- cor(gd)
corrplot(gd, type='upper', tl.srt = 45, tl.col = 'black')
#install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(DMwR)
# Set target to factor
train$mortgage_yn <- as.factor(train$mortgage_yn)
# One-hot endcoding
one_hot <- dummyVars(~ martial_status + education + employment + gender
, data=train, fullRank = T)
train <- cbind(train[, !c('martial_status', 'education', 'employment', 'gender'), with=F]
, predict(one_hot, train))
# We drop the near zero variance variables (mostly dummies)
nzv <- nearZeroVar(train[, !c('mortgage_yn'), with=F])
nzv <- names(train[, !c('mortgage_yn'), with=F])[nzv]
train <- train[, !nzv, with=F]
# Impute missings
# preImp <- preProcess(train, method = 'knnImpute', k=2)
# train <- predict(preImp, train)
# Split to train and test sample
set.seed(1234)
smpl <- createDataPartition(
y = train$mortgage_yn,
p = .75,
list = FALSE
)
training <- train[smpl,]
test <- train[-smpl,]
#rm(train)
fitControl1 <- trainControl(
# 10-fold CV
method = "cv",
number = 3,
classProbs = TRUE,
summaryFunction = twoClassSummary)
fitControl2 <- trainControl(
classProbs = TRUE,
summaryFunction = twoClassSummary)
# Dataset with mean value imputing
dt <- training
dt <- dt[, lapply(.SD, function(x){
if(is.numeric(x)){x[is.na(x)] <- mean(x, na.rm=T)}
return(x)
})]
# Dataset with over- and under-sampling to correct
# the sample imbalance
set.seed(5678)
#Sythetic minority over-sampling
dt <- SMOTE(mortgage_yn ~ ., dt, perc.over = 300, perc.under = 300)
table(dt$mortgage_yn)
# Log transformation of EUR values
dt_glm1 <- copy(dt)
dt_glm1 <- dt_glm1[, ':='
(current_balance_eur = log(current_balance_eur+1)
, cust_income = log(cust_income+1))]
# Plain vanilla logistic regression
m_logit1 <- train(mortgage_yn ~ .,
data = dt_glm1,
method = 'glm',
trControl = fitControl2,
family = 'binomial')
summary(m_logit1$finalModel)
# Transformation of EUR variables
q_bal <- quantile(dt$current_balance_eur, c(.25, .5, .75, 1))
q_inc <- quantile(dt$cust_income, c(.25, .5, .75, 1))
dt_glm2 <- copy(dt)
dt_glm2 <- dt_glm2[, ':='
(current_balance_eur = as.numeric(cut(current_balance_eur
, c(0,q_bal)
, include.lowest = T))
, cust_income = as.numeric(cut(cust_income
, c(0, q_inc)
, include.lowest = T))
)]
m_logit2 <- train(mortgage_yn ~ .,
data = dt_glm2,
method = 'glm',
trControl = fitControl2,
family = 'binomial')
summary(m_logit2$finalModel)
#dt_glm3 <- dt_glm1[, .(mortgage_yn, cust_income, current_balance_eur)]
dt_glm3 <- dt_glm1
dt_glm3$current_balance_eur <- NULL # Balance is dropped
m_logit3 <- train(mortgage_yn ~ .,
data = dt_glm3,
method = 'glm',
trControl = fitControl2,
family = 'binomial')
summary(m_logit3$finalModel)
# Elastic net
g_logit2 <- expand.grid(
alpha = 1
,lambda = seq(0.001, 0.1, by = 0.001))
m_logit4 <- train(mortgage_yn ~ .,
data = dt_glm1,
method = 'glmnet',
trControl = fitControl2,
tuneGrid = g_logit2,
#preProcess = c('scale','center'),
family = 'binomial')
plot(m_logit4)
plot(m_logit4$finalModel)
# Boosted regression tree
dt_xgb <- copy(dt)
mortgage_yn <- dt_xgb$mortgage_yn
target <- ifelse(dt_xgb$mortgage_yn == 'Y',1,0)
dt_xgb$mortgage_yn <- NULL
#dt_xgb$current_balance_eur <- NULL
dt_xgb <- as.matrix(dt_xgb)
# Set up a baseline grid
g_xgb <- expand.grid(
nrounds = seq(10, 50, by = 10),
eta = c(0.025, 0.05, 0.1, 0.3),
max_depth = c(2, 3, 4, 5),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1,
subsample = 1
)
m_xgb1 <- train(x = dt_xgb,
y = mortgage_yn,
trControl = fitControl1,
tuneGrid = g_xgb,
method = 'xgbTree',
preProcess = c('center','scale'),
#verbose = 2,
nthreads = 4
)
plot(m_xgb1)
plot(varImp(m_xgb1))
# Boosted regression tree
dt_xgb2 <- copy(dt)
mortgage_yn <- dt_xgb2$mortgage_yn
target <- ifelse(dt_xgb2$mortgage_yn == 'Y',1,0)
dt_xgb2$mortgage_yn <- NULL
dt_xgb2$current_balance_eur <- NULL
dt_xgb2 <- as.matrix(dt_xgb2)
m_xgb2<- train(x = dt_xgb2,
y = mortgage_yn,
trControl = fitControl1,
tuneGrid = g_xgb,
method = 'xgbTree',
preProcess = c('center','scale'),
#verbose = 2,
nthreads = 4
)
plot(m_xgb2)
plot(varImp(m_xgb2))
# Test set mean imputing
# Dataset with mean value imputing
test[is.na(mths_in_job),]$mths_in_job <- mean(training$mths_in_job, na.rm=T)
test[is.na(mths_with_bank),]$mths_with_bank <- mean(training$mths_with_bank, na.rm=T)
test$cust_income <- log(test$cust_income+1)
p_logit3 <- predict(m_logit3, test)
confusionMatrix(p_logit3, test$mortgage_yn)
p_xgb1 <- predict(m_xgb1, as.matrix(test[,!'mortgage_yn', with=F]), type = 'prob')
confusionMatrix(p_xgb1, as.factor(test$mortgage_yn))
confusionMatrix(p_xgb1, test$mortgage_yn)
p_xgb1 <- predict(m_xgb1, as.matrix(test[,!'mortgage_yn', with=F]))
confusionMatrix(p_xgb1, test$mortgage_yn)
p_xgb2 <- predict(m_xgb2, as.matrix(test[,!'mortgage_yn', with=F]))
confusionMatrix(p_xgb2, test$mortgage_yn)
pred <- fread('../data/potential_customers.csv', sep=';', dec=',')
names(pred) <- tolower(names(pred))
pred[, ':=' (current_address_date = as.Date(current_address_date)
, current_job_date = as.Date(current_job_date)
, current_with_bank_date = as.Date(current_with_bank_date)
)]
print(skim(pred))
pred$age_at_origination <- NA
pred <- pred %>%
feature_intervals() %>%
feature_set_NA()
# One-hot endcoding
test <- cbind(test[, !c('martial_status', 'education', 'employment', 'gender'), with=F]
, predict(one_hot, test))
pred <- cbind(pred[, !c('martial_status', 'education', 'employment', 'gender'), with=F]
, predict(one_hot, pred))
pred
?kmeans
dt_knn <- copy(dt)
dt_knn
dt_knn <- copy(dt)
m_knn <- train(mortgage_yn ~.,
data = dt_knn,
method = "knn",
trControl=fitConbtrol1,
preProcess = c("center", "scale"),
xtuneLength = 10)
dt_knn <- copy(dt)
m_knn <- train(mortgage_yn ~.,
data = dt_knn,
method = "knn",
trControl=fitControl1,
preProcess = c("center", "scale"),
xtuneLength = 10)
dt_knn <- copy(dt)
m_knn <- train(mortgage_yn ~.,
data = dt_knn,
method = "knn",
trControl=fitControl1,
preProcess = c("center", "scale"),
tuneLength = 10)
m_knn
plot(knn)
plot(m_knn)
dt_knn <- copy(dt)
dt_knn$current_balance_eur <- NULL
m_knn <- train(mortgage_yn ~.,
data = dt_knn,
method = "knn",
trControl=fitControl1,
preProcess = c("center", "scale"),
tuneLength = 10)
m_knn
p_knn <- predict(m_knn, test)
confusionMatrix(p_knn, test$mortgage_yn)
p_knn <- predict(m_knn, test)
dim(test)
length(p_knn)
table(p_knn)
#confusionMatrix(p_logit3, test$mortgage_yn)
table(p_logit3)
#confusionMatrix(p_knn, test$mortgage_yn)
table(p_knn)
# Leads
leads <- predict(m_xgb2, pred)
pred
str(m_xgb2)
m_xgb2$finalModel$feature_names
pred <- pred[, m_xgb2$finalModel$feature_names, with=F]
# Leads
leads <- predict(m_xgb2, pred)
leads
sum(leads)
sum(as.numeric(leads))
leads <- as.boolean(leads)
leads <- boolean(leads)
??boolean
leads
str(leads)
as.logical(leads)
as.logical(as.integer(leads) - 1L)
leads <- as.logical(as.integer(leads) - 1L)
ggplot(data=pred[leads,]) +
geom_point(aes(x=cust_income, y=current_balance_eur)) +
scale_x_log10() +
facet_grid(education ~ martial_status, scales = 'free')
pred <- fread('../data/potential_customers.csv', sep=';', dec=',')
names(pred) <- tolower(names(pred))
pred[, ':=' (current_address_date = as.Date(current_address_date)
, current_job_date = as.Date(current_job_date)
, current_with_bank_date = as.Date(current_with_bank_date)
)]
pred_copy = copy(pred)
pred <- fread('../data/potential_customers.csv', sep=';', dec=',')
names(pred) <- tolower(names(pred))
pred[, ':=' (current_address_date = as.Date(current_address_date)
, current_job_date = as.Date(current_job_date)
, current_with_bank_date = as.Date(current_with_bank_date)
)]
pred$age_at_origination <- NA
pred <- pred %>%
feature_intervals() %>%
feature_set_NA()
pred_copy = copy(pred)
ggplot(data=pred_copy[leads,]) +
geom_point(aes(x=cust_income, y=current_balance_eur)) +
scale_x_log10() +
facet_grid(education ~ martial_status, scales = 'free')
library(pROC)
?roc.test
data(aSAH)
roc7 <- roc(aSAH$outcome, aSAH$s100b)
# artificially create an roc8 unpaired with roc7
roc8 <- roc(aSAH$outcome[1:100], aSAH$s100b[1:100])
## Not run:
roc.test(roc7, roc8, paired=FALSE, method="delong")
plot(roc7)
plot(roc8)
plot(roc7)
plot(roc7,roc8)
roc.test
roc7$auc
class(roc7$auc)
roc7$acu$predictor
roc7$auc$predictor
roc7$predictor
roc7$response
roc.test(roc7, roc8, paired=FALSE, method="delong")
roc.test(roc7, roc8, paired=FALSE, method="bootstrap")
roc.test(roc7, roc8, paired=FALSE, method="venkatraman")
roc.test(roc7, roc8, paired=FALSE, method="specificity", specificity=0.9)
setwd("~/work/credit_api")
library(data.table)
library(skimr)
library(mltools)
# Load data -----
# Note: here we assume that the data extraction and integration is
# already done.
dev <- fread('data/application_train.csv', stringsAsFactors = FALSE)
names(dev) <- tolower(names(dev))
descr <- fread('data/HomeCredit_columns_description.csv', stringsAsFactors = FALSE)
descr <- descr[Table == 'application_{train|test}.csv', ]
# Data exploration
skim_dev <- data.table(skim(dev))
# Quickly explore character variables ----
cols <- unique(skim_dev[skim_dev$type == 'character']$variable)
# We drop two flags from the list (see later)
cols <- cols[!(cols %in% c('flag_own_realty','flag_own_car'))]
skim_dev_chr <- data.table()
for(i in cols){
r <- dev[, .(.N), by = i]
names(r)[1] <- "value"
r$var <- i
skim_dev_chr <- rbind(skim_dev_chr, r[,.(var, value, N)])
}
# Practically the empty fields and the XNA values are the missing ones
# so we convert these values to missing
dev[, (cols) := lapply(.SD, function(x) ifelse(x %in% c('XNA',''), '', x))
, .SDcols = cols]
dev[, (cols) := lapply(.SD, function(x) factor(as.numeric(factor(x)), ordered = F))
, .SDcols = cols]
# One_hot encoding of categrical variables
dev <- one_hot(dev, naCols=TRUE)
rm('r', 'cols') # cleanup
# Quickly explore flag variables ----
# Recode the flag_own_car and flag_own_reality to 0/1
dev$flag_own_car <- ifelse(dev$flag_own_car == 'N', as.integer(0), as.integer(1))
dev$flag_own_realty <- ifelse(dev$flag_own_realty == 'N', as.integer(0), as.integer(1))
# Some of them are flags while others are counters
# Let's identify first the flag variables using the metatdata descr
cols <- tolower(descr[substr(Row,1,4) == 'FLAG' | substr(Description,1,4) == 'Flag', ]$Row)
skim(dev[, (cols), with=F])
rm('cols')
# Quickly explore numeric variables ----
# Our general rule is that if the missing ratio is above 30%
# then we drop the variable
skim_dev_num <- skim_dev[type == 'numeric' & stat %in% c('missing','n'), .(variable, stat, value)]
skim_dev_num <- dcast(skim_dev_num, variable ~ stat)
skim_dev_num[, rat_missing := missing/n]
# Drop features from the dev sample ----
drop <- c('sk_id_curr'
,'fondkapremont_mode'
,'flag_cont_mobile'
,'flag_mobil'
# ,'ext_source_1'
# ,'ext_source_2'
# ,'ext_source_3'
, skim_dev_num[rat_missing>0.3,]$variable
)
drop <- unique(drop)
dev[, (drop) := NULL]
rm(drop)
# Create test sample ----
smpl_size <- floor(0.75 * nrow(dev))
set.seed(12345)
dev_ids <- sample(seq_len(nrow(dev)), size = smpl_size, replace = FALSE)
test <- dev[-dev_ids,]
dev <- dev[dev_ids,]
# Save samples ----
save(test, file = 'stage/test.Rdata')
save(dev, file ='stage/dev.Rdata')
knit_with_parameters('~/work/credit_api/explore.Rmd')
library(rmarkdown)
rmarkdow::render('explore.Rmd')
rmarkdown::render('explore.Rmd')
?rmarkdown::render
rmarkdown::render('explore.Rmd', output_dir = 'artifacts')
getwd()
rmarkdown::render('explore.Rmd', output_dir = 'artifacts')
rmarkdown::render('explore.Rmd')
rmarkdown::render('explore.Rmd', output_dir = 'artifacts')
rmarkdown::render('explore.Rmd', output_dir = 'artifacts')
